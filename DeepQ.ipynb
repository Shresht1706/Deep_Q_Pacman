{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (2.2.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (2.2.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: ale_py>=0.9 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (0.10.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.1.1 does not provide the extra 'accept-rom-license'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale-py in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: numpy>1.20 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ale-py) (2.2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#use': Expected package name at the start of dependency specifier\n",
      "    #use\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (2.2.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\shres\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (4.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "!pip install ale-py\n",
    "!pip install swig #use choco,apt-get or pip depending on os and system\n",
    "!pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, action_size, seed = 42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "  def __init__(self, action_size, seed = 42):\n",
    "    super(Network, self).__init__()\n",
    "    self.seed = torch.manual_seed(seed)\n",
    "    self.conv1 = nn.Conv2d(3, 32, kernel_size = 8, stride = 4)\n",
    "    self.bn1 = nn.BatchNorm2d(32)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride = 2)\n",
    "    self.bn2 = nn.BatchNorm2d(64)\n",
    "    self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1)\n",
    "    self.bn3 = nn.BatchNorm2d(64)\n",
    "    self.conv4 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1)\n",
    "    self.bn4 = nn.BatchNorm2d(128)\n",
    "    self.fc1 = nn.Linear(10 * 10 * 128, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.fc3 = nn.Linear(256, action_size)\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = F.relu(self.bn1(self.conv1(state)))\n",
    "    x = F.relu(self.bn2(self.conv2(x)))\n",
    "    x = F.relu(self.bn3(self.conv3(x)))\n",
    "    x = F.relu(self.bn4(self.conv4(x)))\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shresht/anaconda3/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (210, 160, 3)\n",
      "State size:  210\n",
      "Number of actions:  9\n"
     ]
    }
   ],
   "source": [
    "import ale_py\n",
    "import gymnasium as gym\n",
    "env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "number_actions = env.action_space.n\n",
    "print('State shape: ', state_shape)\n",
    "print('State size: ', state_size)\n",
    "print('Number of actions: ', number_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "minibatch_size = 64\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "  frame = Image.fromarray(frame)\n",
    "  preprocess = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
    "  return preprocess(frame).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "  def __init__(self, action_size):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.action_size = action_size\n",
    "    self.local_qnetwork = Network(action_size).to(self.device)\n",
    "    self.target_qnetwork = Network(action_size).to(self.device)\n",
    "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
    "    self.memory = deque(maxlen = 10000)\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    state = preprocess_frame(state)\n",
    "    next_state = preprocess_frame(next_state)\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "    if len(self.memory) > minibatch_size:\n",
    "      experiences = random.sample(self.memory, k = minibatch_size)\n",
    "      self.learn(experiences, discount_factor)\n",
    "\n",
    "  def act(self, state, epsilon = 0.):\n",
    "    state = preprocess_frame(state).to(self.device)\n",
    "    self.local_qnetwork.eval()\n",
    "    with torch.no_grad():\n",
    "      action_values = self.local_qnetwork(state)\n",
    "    self.local_qnetwork.train()\n",
    "    if random.random() > epsilon:\n",
    "      return np.argmax(action_values.cpu().data.numpy())\n",
    "    else:\n",
    "      return random.choice(np.arange(self.action_size))\n",
    "\n",
    "  def learn(self, experiences, discount_factor):\n",
    "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "    states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n",
    "    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
    "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
    "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n",
    "    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n",
    "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
    "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "    loss = F.mse_loss(q_expected, q_targets)\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(number_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: 289.50"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state, epsilon)\n\u001b[1;32m     14\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 15\u001b[0m agent\u001b[38;5;241m.\u001b[39mstep(state, action, reward, next_state, done)\n\u001b[1;32m     16\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     17\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m minibatch_size:\n\u001b[1;32m     16\u001b[0m   experiences \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory, k \u001b[38;5;241m=\u001b[39m minibatch_size)\n\u001b[0;32m---> 17\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn(experiences, discount_factor)\n",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self, experiences, discount_factor)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(q_expected, q_targets)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_episodes = 2000\n",
    "maximum_number_timesteps_per_episode = 10000\n",
    "epsilon_starting_value  = 1.0\n",
    "epsilon_ending_value  = 0.01\n",
    "epsilon_decay_value  = 0.995\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen = 100)\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "  state, _ = env.reset()\n",
    "  score = 0\n",
    "  for t in range(maximum_number_timesteps_per_episode):\n",
    "    action = agent.act(state, epsilon)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    agent.step(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    if done:\n",
    "      break\n",
    "  scores_on_100_episodes.append(score)\n",
    "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
    "  if episode % 100 == 0:\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
    "  if np.mean(scores_on_100_episodes) >= 500.0:\n",
    "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
    "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shresht/anaconda3/lib/python3.11/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/Users/shresht/anaconda3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TiffWriter.write() got an unexpected keyword argument 'fps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     18\u001b[0m     imageio\u001b[38;5;241m.\u001b[39mmimsave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m show_video_of_model(agent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMsPacmanDeterministic-v0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_video\u001b[39m():\n\u001b[1;32m     23\u001b[0m     mp4list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m, in \u001b[0;36mshow_video_of_model\u001b[0;34m(agent, env_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m     state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     17\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 18\u001b[0m imageio\u001b[38;5;241m.\u001b[39mmimsave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imageio/v2.py:495\u001b[0m, in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mwrite(ims, is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/imageio/plugins/tifffile_v3.py:244\u001b[0m, in \u001b[0;36mTifffilePlugin.write\u001b[0;34m(self, ndimage, is_batch, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m     ndimage \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(ndimage)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m ndimage:\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh\u001b[38;5;241m.\u001b[39mwrite(image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request\u001b[38;5;241m.\u001b[39m_uri_type \u001b[38;5;241m==\u001b[39m URI_BYTES:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mTypeError\u001b[0m: TiffWriter.write() got an unexpected keyword argument 'fps'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "    env.close()\n",
    "    imageio.mimsave('video.mp4', frames, fps=30)\n",
    "\n",
    "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
